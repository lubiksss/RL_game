{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975ea893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.image as img\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492c4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-8-4-v0')\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [['right'], ['right', 'A']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d24a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque as dq\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NN를 학습시키기 위한 hyperparameter\n",
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "\n",
    "# 감마는 할인율이라고 부르는 값으로, 미래가치에 대한 중요도를 조절합니다.\n",
    "# 클수록 미래에 받을 보상에 더 큰 가치를 두는 것.\n",
    "gamma = 0.98\n",
    "\n",
    "buffer_limit = 50000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2d328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강화학습은 Training data set이라는게 따로 없다. Agent가 행동을 취하고 데이터셋을 쌓아나가야합니다.\n",
    "# 그 데이터셋을 쌓기 위한 버퍼\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = dq(maxlen=buffer_limit)\n",
    "    \n",
    "    # 버퍼에는 (state, action ,reward, nstate, done) 값이 들어갑니다.\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    # 샘플 함수를 만드는 이유는 버퍼에 쌓인 데이터셋에서 랜덤으로 학습을 시키기 위함입니다.\n",
    "    # 그냥 연속해서 쌓인 n개의 데이터셋을 그대로 사용하면 데이터간의 상관관계가 너무 크기 때문에 학슴이 잘 안됩니다.\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float, device=device), torch.tensor(a_lst, device=device), \\\n",
    "               torch.tensor(r_lst, dtype=torch.float, device=device), torch.tensor(s_prime_lst, dtype=torch.float, device=device), \\\n",
    "               torch.tensor(done_mask_lst, dtype=torch.float, device=device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81049df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole의 state가 4개고 action은 2개이기 때문에 input 4, output 2인 NN생성\n",
    "# 2층짜리 NN입니다. 임의로 설계했습니다.\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(55632, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1,55632)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.float()\n",
    "    \n",
    "    # epsilon greedy 전략을 사용합니다.\n",
    "    # 간단하게 설명하면 탐험이라는 개념을 통해서 가보지 않은 경로를 가볼 수 있게 해줍니다.\n",
    "    def sample_action(self, observation, epsilon):\n",
    "        out = self.forward(observation)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else : \n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df872664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "        \n",
    "        # 벨만함수로부터 유도된 DQN 비용함수를 구현 학습시킵니다.\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1,a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fb7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed39c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAEDEOK_HOME\\anaconda3\\envs\\RL_env\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "n_episode :20, score : 1644.2, n_buffer : 50000, eps : 7.9%\n"
     ]
    }
   ],
   "source": [
    "# Double Deep Q Learning 개념\n",
    "# target_net을 semi constant로 사용\n",
    "q = Qnet().to(device)\n",
    "q_target = Qnet().to(device)\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "print_interval = 20\n",
    "score = 0.0  \n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_epi in range(20+1):\n",
    "    print(n_epi)\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "    s = env.reset()\n",
    "    s = Image.fromarray(s).convert('L')\n",
    "    s = np.array(s, 'uint8')\n",
    "    s = s.reshape((1,)+s.shape)\n",
    "    s = s.copy()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = q.sample_action(torch.from_numpy(s.reshape((1,)+s.shape)).float(), epsilon)      \n",
    "        s_prime, r, done, info = env.step(a)\n",
    "        \n",
    "        s_prime = Image.fromarray(s_prime).convert('L')\n",
    "        s_prime = np.array(s_prime, 'uint8')\n",
    "        s_prime = s_prime.reshape((1,)+s_prime.shape)\n",
    "        s_prime = s_prime.copy()\n",
    "        \n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "        s = s_prime\n",
    "\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # 메모리가 어느정도 차야 random sample이 가능하기 때문에 일정 이상 차면 학습을 진행\n",
    "    if memory.size()>2000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        # 일정 주기마다 semi constant인 target-net도 업데이트.\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                        n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "        score = 0.0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423bbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q.state_dict(), 'Mario_weight.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
