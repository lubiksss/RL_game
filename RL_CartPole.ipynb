{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d24a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque as dq\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NN를 학습시키기 위한 hyperparameter\n",
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "\n",
    "# 감마는 할인율이라고 부르는 값으로, 미래가치에 대한 중요도를 조절합니다.\n",
    "# 클수록 미래에 받을 보상에 더 큰 가치를 두는 것.\n",
    "gamma = 0.98\n",
    "\n",
    "buffer_limit = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2d328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강화학습은 Training data set이라는게 따로 없다. Agent가 행동을 취하고 데이터셋을 쌓아나가야합니다.\n",
    "# 그 데이터셋을 쌓기 위한 버퍼\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = dq(maxlen=buffer_limit)\n",
    "    \n",
    "    # 버퍼에는 (state, action ,reward, nstate, done) 값이 들어갑니다.\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    # 샘플 함수를 만드는 이유는 버퍼에 쌓인 데이터셋에서 랜덤으로 학습을 시키기 위함입니다.\n",
    "    # 그냥 연속해서 쌓인 n개의 데이터셋을 그대로 사용하면 데이터간의 상관관계가 너무 크기 때문에 학슴이 잘 안됩니다.\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81049df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole의 state가 4개고 action은 2개이기 때문에 input 4, output 2인 NN생성\n",
    "# 2층짜리 NN입니다. 임의로 설계했습니다.\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # epsilon greedy 전략을 사용합니다.\n",
    "    # 간단하게 설명하면 탐험이라는 개념을 통해서 가보지 않은 경로를 가볼 수 있게 해줍니다.\n",
    "    def sample_action(self, observation, epsilon):\n",
    "        out = self.forward(observation)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else : \n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df872664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "        \n",
    "        # 벨만함수로부터 유도된 DQN 비용함수를 구현 학습시킵니다.\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1,a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef5fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f65d032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5011,  0.5045],\n",
       "        [-0.2605, -0.3426],\n",
       "        [ 0.4312,  0.3976],\n",
       "        [ 0.0009,  0.0116],\n",
       "        [ 0.2957,  0.3419],\n",
       "        [ 0.3485,  0.3526],\n",
       "        [ 0.4578,  0.4493],\n",
       "        [ 0.1231, -0.0405],\n",
       "        [ 0.3229,  0.3123],\n",
       "        [-0.3035, -0.1345],\n",
       "        [ 0.4271,  0.4495],\n",
       "        [ 0.4439,  0.4244],\n",
       "        [ 0.4272,  0.4253],\n",
       "        [ 0.4729,  0.4824],\n",
       "        [ 0.5111,  0.5068],\n",
       "        [ 0.4655,  0.4129],\n",
       "        [ 0.2215,  0.2979],\n",
       "        [ 0.1017,  0.0941],\n",
       "        [ 0.2221,  0.0929],\n",
       "        [ 0.1944,  0.3051],\n",
       "        [ 0.4375,  0.4476],\n",
       "        [ 0.4465,  0.4589],\n",
       "        [-0.0619, -0.1982],\n",
       "        [ 0.3953,  0.4003],\n",
       "        [-0.1845, -0.2624],\n",
       "        [-0.2134, -0.3198],\n",
       "        [ 0.4349,  0.3959],\n",
       "        [ 0.4640,  0.4376],\n",
       "        [ 0.3767,  0.4795],\n",
       "        [ 0.3531,  0.3739],\n",
       "        [ 0.0128,  0.0234],\n",
       "        [ 0.1802,  0.2161]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed39c84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :20, score : 9.9, n_buffer : 199, eps : 7.9%\n",
      "n_episode :40, score : 9.5, n_buffer : 389, eps : 7.8%\n",
      "n_episode :60, score : 9.8, n_buffer : 586, eps : 7.7%\n",
      "n_episode :80, score : 10.1, n_buffer : 787, eps : 7.6%\n",
      "n_episode :100, score : 10.1, n_buffer : 988, eps : 7.5%\n",
      "n_episode :120, score : 9.7, n_buffer : 1182, eps : 7.4%\n",
      "n_episode :140, score : 9.7, n_buffer : 1376, eps : 7.3%\n",
      "n_episode :160, score : 9.4, n_buffer : 1565, eps : 7.2%\n",
      "n_episode :180, score : 9.7, n_buffer : 1759, eps : 7.1%\n",
      "n_episode :200, score : 9.8, n_buffer : 1956, eps : 7.0%\n",
      "n_episode :220, score : 18.9, n_buffer : 2334, eps : 6.9%\n",
      "n_episode :240, score : 18.3, n_buffer : 2700, eps : 6.8%\n",
      "n_episode :260, score : 21.5, n_buffer : 3130, eps : 6.7%\n",
      "n_episode :280, score : 20.5, n_buffer : 3540, eps : 6.6%\n",
      "n_episode :300, score : 17.6, n_buffer : 3891, eps : 6.5%\n",
      "n_episode :320, score : 19.5, n_buffer : 4281, eps : 6.4%\n",
      "n_episode :340, score : 23.2, n_buffer : 4746, eps : 6.3%\n",
      "n_episode :360, score : 23.4, n_buffer : 5215, eps : 6.2%\n",
      "n_episode :380, score : 26.9, n_buffer : 5754, eps : 6.1%\n",
      "n_episode :400, score : 52.6, n_buffer : 6807, eps : 6.0%\n",
      "n_episode :420, score : 41.6, n_buffer : 7640, eps : 5.9%\n",
      "n_episode :440, score : 73.6, n_buffer : 9112, eps : 5.8%\n",
      "n_episode :460, score : 77.1, n_buffer : 10654, eps : 5.7%\n",
      "n_episode :480, score : 150.4, n_buffer : 13662, eps : 5.6%\n",
      "n_episode :500, score : 188.9, n_buffer : 17441, eps : 5.5%\n",
      "n_episode :520, score : 223.2, n_buffer : 21904, eps : 5.4%\n",
      "n_episode :540, score : 245.1, n_buffer : 26805, eps : 5.3%\n",
      "n_episode :560, score : 209.6, n_buffer : 30996, eps : 5.2%\n",
      "n_episode :580, score : 245.2, n_buffer : 35900, eps : 5.1%\n",
      "n_episode :600, score : 213.9, n_buffer : 40179, eps : 5.0%\n",
      "n_episode :620, score : 221.3, n_buffer : 44605, eps : 4.9%\n",
      "n_episode :640, score : 188.2, n_buffer : 48368, eps : 4.8%\n",
      "n_episode :660, score : 213.9, n_buffer : 50000, eps : 4.7%\n",
      "n_episode :680, score : 241.0, n_buffer : 50000, eps : 4.6%\n",
      "n_episode :700, score : 251.4, n_buffer : 50000, eps : 4.5%\n",
      "n_episode :720, score : 175.6, n_buffer : 50000, eps : 4.4%\n",
      "n_episode :740, score : 213.2, n_buffer : 50000, eps : 4.3%\n",
      "n_episode :760, score : 287.8, n_buffer : 50000, eps : 4.2%\n",
      "n_episode :780, score : 252.7, n_buffer : 50000, eps : 4.1%\n",
      "n_episode :800, score : 281.2, n_buffer : 50000, eps : 4.0%\n",
      "n_episode :820, score : 257.8, n_buffer : 50000, eps : 3.9%\n",
      "n_episode :840, score : 164.8, n_buffer : 50000, eps : 3.8%\n",
      "n_episode :860, score : 293.4, n_buffer : 50000, eps : 3.7%\n",
      "n_episode :880, score : 229.8, n_buffer : 50000, eps : 3.6%\n",
      "n_episode :900, score : 237.7, n_buffer : 50000, eps : 3.5%\n",
      "n_episode :920, score : 180.3, n_buffer : 50000, eps : 3.4%\n",
      "n_episode :940, score : 179.3, n_buffer : 50000, eps : 3.3%\n",
      "n_episode :960, score : 263.4, n_buffer : 50000, eps : 3.2%\n",
      "n_episode :980, score : 318.4, n_buffer : 50000, eps : 3.1%\n",
      "n_episode :1000, score : 248.4, n_buffer : 50000, eps : 3.0%\n",
      "n_episode :1020, score : 286.0, n_buffer : 50000, eps : 2.9%\n",
      "n_episode :1040, score : 275.2, n_buffer : 50000, eps : 2.8%\n",
      "n_episode :1060, score : 328.4, n_buffer : 50000, eps : 2.7%\n",
      "n_episode :1080, score : 241.8, n_buffer : 50000, eps : 2.6%\n",
      "n_episode :1100, score : 300.9, n_buffer : 50000, eps : 2.5%\n",
      "n_episode :1120, score : 269.4, n_buffer : 50000, eps : 2.4%\n",
      "n_episode :1140, score : 249.2, n_buffer : 50000, eps : 2.3%\n",
      "n_episode :1160, score : 95.5, n_buffer : 50000, eps : 2.2%\n",
      "n_episode :1180, score : 123.7, n_buffer : 50000, eps : 2.1%\n",
      "n_episode :1200, score : 252.2, n_buffer : 50000, eps : 2.0%\n",
      "n_episode :1220, score : 322.8, n_buffer : 50000, eps : 1.9%\n",
      "n_episode :1240, score : 290.1, n_buffer : 50000, eps : 1.8%\n",
      "n_episode :1260, score : 263.6, n_buffer : 50000, eps : 1.7%\n",
      "n_episode :1280, score : 172.3, n_buffer : 50000, eps : 1.6%\n",
      "n_episode :1300, score : 270.4, n_buffer : 50000, eps : 1.5%\n",
      "n_episode :1320, score : 233.7, n_buffer : 50000, eps : 1.4%\n",
      "n_episode :1340, score : 134.8, n_buffer : 50000, eps : 1.3%\n",
      "n_episode :1360, score : 212.3, n_buffer : 50000, eps : 1.2%\n",
      "n_episode :1380, score : 270.1, n_buffer : 50000, eps : 1.1%\n",
      "n_episode :1400, score : 334.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1420, score : 284.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1440, score : 226.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1460, score : 275.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1480, score : 165.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1500, score : 222.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1520, score : 149.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1540, score : 131.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1560, score : 132.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1580, score : 149.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1600, score : 200.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1620, score : 214.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1640, score : 166.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1660, score : 177.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1680, score : 301.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1700, score : 132.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1720, score : 189.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1740, score : 121.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1760, score : 169.5, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1780, score : 256.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1800, score : 148.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1820, score : 150.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1840, score : 186.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1860, score : 227.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1880, score : 240.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1900, score : 271.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1920, score : 212.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1940, score : 231.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1960, score : 203.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1980, score : 201.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2000, score : 173.3, n_buffer : 50000, eps : 1.0%\n"
     ]
    }
   ],
   "source": [
    "# Double Deep Q Learning 개념\n",
    "# target_net을 semi constant로 사용\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "print_interval = 20\n",
    "score = 0.0  \n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_epi in range(2000+1):\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "        s_prime, r, done, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "        s = s_prime\n",
    "\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # 메모리가 어느정도 차야 random sample이 가능하기 때문에 일정 이상 차면 학습을 진행\n",
    "    if memory.size()>2000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        # 일정 주기마다 semi constant인 target-net도 업데이트.\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                        n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "        score = 0.0\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc4e665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q.state_dict(), 'Cartpole_weight.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
